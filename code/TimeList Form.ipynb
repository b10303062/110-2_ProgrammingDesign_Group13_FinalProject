{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定日期區間\n",
    "\n",
    "import datetime\n",
    "\n",
    "def dt_to_str(date_obj):\n",
    "    return '{:04}'.format(date_obj.year)+'-'+'{:02}'.format(date_obj.month)+'-'+'{:02}'.format(date_obj.day)\n",
    "\n",
    "def str_to_dt(string):\n",
    "    word = string.split('-')\n",
    "    y = int(word[0])\n",
    "    m = int(word[1])\n",
    "    d = int(word[2])\n",
    "    return datetime.date(y, m, d)\n",
    "\n",
    "start = datetime.date(2022, 3, 29)\n",
    "end = datetime.date(2022, 5, 13)\n",
    "time = start\n",
    "lst = []\n",
    "\n",
    "#donnot_get = ['2022-05-01']\n",
    "\n",
    "while time <= (end + datetime.timedelta(1)):\n",
    "    lst += [time]\n",
    "    time += datetime.timedelta(1)\n",
    "\n",
    "#print(lst)\n",
    "\n",
    "days = [dt_to_str(d) for d in lst]\n",
    "\n",
    "#print(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 處理車次列表 (現成檔案)\n",
    "import json\n",
    "\n",
    "with open('TRATimeTable_2022-03-29_0301.json', 'r', encoding='utf8') as f:\n",
    "    json_file = json.load(f)\n",
    "\n",
    "with open('Exclude.txt', 'r', encoding='utf8') as f:\n",
    "    exclude = []\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            data = line.strip().split('：')[-1].split(chr(9))  #拆分車次資料\n",
    "            for d in data:\n",
    "                exclude += [d]\n",
    "        except:\n",
    "            break\n",
    "\n",
    "ids = []\n",
    "\n",
    "for train in json_file[\"TrainTimetables\"]:\n",
    "    i = train[\"GeneralTrainInfo\"][\"TrainNo\"]\n",
    "    if i not in exclude and i[-1].isalpha() == False:\n",
    "        ids += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ids))\n",
    "print(exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抓取歷史紀錄 (現成檔案)\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "database = []\n",
    "for d, tq in zip(days, tqdm(days)):\n",
    "    route = f'.\\\\DailyDelayInfo\\\\{d}.csv'\n",
    "    with open(route, 'r', encoding='utf8') as csv_file:\n",
    "        whole_file = list(csv.reader(csv_file))\n",
    "        title = whole_file[0]\n",
    "    database += [[d[0], d[2], d[4], d[5]] for d in whole_file[1:]]\n",
    "days = days[:-1]  #將原本多加進來的一天去除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取站名資料 (現成檔案)\n",
    "with open('Stations.txt', 'r', encoding='utf8') as f:\n",
    "    sta_lst = f.readlines()\n",
    "    sta_lst = [i.strip() for i in sta_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecd07fbb3694e18bffc3e2194ae98e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/683 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 讀取、轉換資料\n",
    "\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "import json\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "#ids = ['666']\n",
    "\n",
    "def time(string):\n",
    "    if string[-2:] == '.5':\n",
    "        second = 30\n",
    "        string = string.replace('.5', '')\n",
    "    else:\n",
    "        second = 0\n",
    "    hour = int(string.split(':')[0])\n",
    "    minute = int(string.split(':')[1])\n",
    "    return datetime.time(hour,minute,second)\n",
    "\n",
    "#for ID in ids:\n",
    "for ID, tq in zip(ids, tqdm(ids)):\n",
    "    for i, train in enumerate(json_file[\"TrainTimetables\"]):\n",
    "        if train[\"GeneralTrainInfo\"][\"TrainNo\"] == ID:\n",
    "            for sta in train[\"Stations\"]:\n",
    "                if sta[\"StationName\"] == (train[\"GeneralTrainInfo\"][\"StartingStationName\"] if train[\"GeneralTrainInfo\"][\"StartingServiceStationName\"] == '' else train[\"GeneralTrainInfo\"][\"StartingServiceStationName\"]):\n",
    "                    start_time = time(sta[\"DepartureTime\"])\n",
    "                    break\n",
    "            sta_record_order = []\n",
    "            for k in train[\"Stations\"]:\n",
    "                if k[\"StationName\"] in sta_lst:\n",
    "                    sta_record_order += [k[\"StationName\"]]\n",
    "            \n",
    "            sta_record_order = sta_record_order[:-1]\n",
    "\n",
    "            # 刪除原本用json資料處理起終點與環島遇到重複站的內容\n",
    "            # 注意！因為紀錄只到倒數第二站，sta_record_order 直接去掉最後一站\n",
    "    \n",
    "    delay_data_lst = []\n",
    "    this_train_data = []\n",
    "\n",
    "    for line in database:   #挑出當次車的資料\n",
    "        if line[0] == ID:\n",
    "            this_train_data += [{'StationNameZh_tw':line[1], \\\n",
    "                                'DelayTime':int(line[2]), \\\n",
    "                                'SrcUpdateTime':line[3]}]\n",
    "\n",
    "    def record_time_transfer(string):\n",
    "        return datetime.datetime.strptime(string, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "\n",
    "    this_train_data.sort(key=lambda ttd: record_time_transfer(ttd['SrcUpdateTime']))\n",
    "    daily_data = {}\n",
    "\n",
    "    for day in days:      #挑出各日期的資料\n",
    "        for i, ttd in enumerate(this_train_data):\n",
    "            the_date_of_record = record_time_transfer(ttd['SrcUpdateTime']).date()\n",
    "            dt_day = datetime.datetime.strptime(day, \"%Y-%m-%d\").date()\n",
    "            record_time = record_time_transfer(ttd['SrcUpdateTime'])\n",
    "            record_time = record_time.replace(tzinfo=None)\n",
    "            if the_date_of_record == dt_day and \\\n",
    "                record_time >= datetime.datetime.combine(dt_day, start_time):\n",
    "                daily_data[day] = []\n",
    "                ind = i\n",
    "                while True:\n",
    "                    daily_data[day] += [this_train_data[ind]]\n",
    "                    #now_time = record_time_transfer(this_train_data[i]['SrcUpdateTime'])\n",
    "                    if ind+1 == len(this_train_data):\n",
    "                        break\n",
    "                    next_time = record_time_transfer(this_train_data[ind+1]['SrcUpdateTime'])\n",
    "                    next_time = next_time.replace(tzinfo=None)\n",
    "                    if next_time >= datetime.datetime.combine(dt_day+datetime.timedelta(1), start_time):\n",
    "                        break\n",
    "                    else:\n",
    "                        ind += 1\n",
    "                break\n",
    "\n",
    "    if len(daily_data) == 0:\n",
    "        print(f'No data in TrainNo {ID}')\n",
    "        continue\n",
    "\n",
    "    repeat_data_removed = {}\n",
    "\n",
    "    for day in days:     #依站序抓取資料\n",
    "        if day not in daily_data:\n",
    "            continue\n",
    "        tmp_data = []\n",
    "        for sta in sta_record_order:\n",
    "            record = []\n",
    "            for data in daily_data[day]:\n",
    "                if data['StationNameZh_tw'] == sta:\n",
    "                    record += [data]\n",
    "            delay_time = -1   #初始值、異常值\n",
    "            if len(record) == 0:\n",
    "                tmp_data += [{'StationNameZh_tw':sta, \\\n",
    "                                'DelayTime':-1, \\\n",
    "                                'SrcUpdateTime':\"\"}]\n",
    "            else:\n",
    "                for r in record:\n",
    "                    if r['DelayTime'] > delay_time:\n",
    "                        this_sta_data = r\n",
    "                        delay_time = r['DelayTime']\n",
    "                tmp_data += [r]\n",
    "        repeat_data_removed[day] = tmp_data\n",
    "\n",
    "    if True:\n",
    "        with open(f'.\\\\Cleaned\\\\{ID}.json','w',encoding='utf8') as f:\n",
    "            json.dump(repeat_data_removed, f, indent=4, ensure_ascii=False)\n",
    "    else:\n",
    "        pprint(repeat_data_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
